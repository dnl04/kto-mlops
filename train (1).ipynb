{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21048dc5-470b-4f10-a6b1-037b7c673646",
   "metadata": {},
   "source": [
    "# Create s3 client in order to download and upload data from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7aa7a4-fdbe-47b8-9af8-c738d7cb6215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.36.16-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.37.0,>=1.36.16 (from boto3)\n",
      "  Downloading botocore-1.36.16-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/vscode/.local/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.16->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/vscode/.local/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.16->boto3) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.16->boto3) (1.17.0)\n",
      "Downloading boto3-1.36.16-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.36.16-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.36.16 botocore-1.36.16 jmespath-1.0.1 s3transfer-0.11.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f69a6ec-b896-49f0-991c-87cfc69d15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio-api-ldanois17-dev.apps.rm1.0a51.p1.openshiftapps.com\",\n",
    "    aws_access_key_id=\"minio\",\n",
    "    aws_secret_access_key=\"minio123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0887a4-8431-4dd6-92c2-aab64a35db1f",
   "metadata": {},
   "source": [
    "# Extract labels from annotations file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da6dc42-d692-4107-b13f-de316cf20e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "def extraction_from_annotation_file(bucket_name: str, s3_path: str, filename: str, s3_client) -> tuple[dict[Any, Any], set[Any]]:\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    s3_client.download_file(bucket_name, s3_path, filename)\n",
    "\n",
    "    extract = {}\n",
    "    classes = set()\n",
    "    with open(filename) as file:\n",
    "        annotations = json.load(file)[\"annotations\"]\n",
    "        for annotation in annotations:\n",
    "            label = annotation[\"annotation\"][\"label\"]\n",
    "            extract[annotation[\"fileName\"]] = label\n",
    "            classes.add(label)\n",
    "    return extract, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bf902-2266-41c5-9d34-556b6595b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"./dist\"\n",
    "bucket_name = \"cats-dogs-other\"\n",
    "extract, classes = extraction_from_annotation_file(bucket_name, \n",
    "                                                    \"dataset/cats_dogs_others-annotations.json\",\n",
    "                                                    working_dir + \"/cats_dogs_others-annotations.json\",\n",
    "                                                    s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef163bd0-e9a9-4fb3-b007-1f1ecf845ca2",
   "metadata": {},
   "source": [
    "# Random split train / evaluate / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736f6cc-8a5d-4859-a5c6-1486f369c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = working_dir + \"/train\"\n",
    "evaluate_dir = working_dir + \"/evaluate\"\n",
    "test_dir = working_dir + \"/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4c758-3c00-485c-84a7-12b59e342bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def random_split_train_evaluate_test_from_extraction(extract: dict,\n",
    "                                                     classes: set,\n",
    "                                                     split_ratio_train: float,\n",
    "                                                     split_ratio_evaluate: float,\n",
    "                                                     split_ratio_test: float,\n",
    "                                                     train_dir: str,\n",
    "                                                     evaluate_dir: str,\n",
    "                                                     test_dir: str,\n",
    "                                                     bucket_name: str,\n",
    "                                                     s3_path: str,\n",
    "                                                     s3_client):\n",
    "\n",
    "    if split_ratio_train + split_ratio_evaluate + split_ratio_test != 1:\n",
    "        raise Exception(\"sum of ratio must be equal to 1\")\n",
    "\n",
    "    keys_list = list(extract.keys())  # shuffle() wants a list\n",
    "    random.shuffle(keys_list)  # randomize the order of the keys\n",
    "\n",
    "    nkeys_train = int(split_ratio_train * len(keys_list))  # how many keys does split ratio train% equal\n",
    "    keys_train = keys_list[:nkeys_train]\n",
    "    keys_evaluate_and_test = keys_list[nkeys_train:]\n",
    "\n",
    "    split_ratio_evaluate_and_test = split_ratio_evaluate + split_ratio_test\n",
    "    nkeys_evaluate = int((split_ratio_evaluate / split_ratio_evaluate_and_test) * len(keys_evaluate_and_test))\n",
    "    keys_evaluate = keys_evaluate_and_test[:nkeys_evaluate]\n",
    "    keys_test = keys_evaluate_and_test[nkeys_evaluate:]\n",
    "\n",
    "    extract_train = {k: extract[k] for k in keys_train}\n",
    "    extract_evaluate = {k: extract[k] for k in keys_evaluate}\n",
    "    extract_test = {k: extract[k] for k in keys_test}\n",
    "\n",
    "    # create directories\n",
    "    for existing_class in classes:\n",
    "        Path(train_dir + \"/\" + existing_class).mkdir(parents=True, exist_ok=True)\n",
    "        Path(evaluate_dir + \"/\" + existing_class).mkdir(parents=True, exist_ok=True)\n",
    "        Path(test_dir + \"/\" + existing_class).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # add files in directories\n",
    "    download_files(extract_train, train_dir, bucket_name, s3_path, s3_client)\n",
    "    download_files(extract_evaluate, evaluate_dir, bucket_name, s3_path, s3_client)\n",
    "    download_files(extract_test, test_dir, bucket_name, s3_path, s3_client)\n",
    "\n",
    "\n",
    "def download_files(extract: dict, directory: str, bucket_name: str, s3_path: str, s3_client):\n",
    "    for key, value in extract.items():\n",
    "        s3_client.download_file(bucket_name, s3_path + key, directory + \"/\" + value + \"/\" + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b9d84-1fa3-4ed1-8a63-9395fcbcb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_train = 0.8\n",
    "split_ratio_evaluate = 0.1\n",
    "split_ratio_test = 0.1\n",
    "\n",
    "random_split_train_evaluate_test_from_extraction(extract, classes, split_ratio_train,\n",
    "                                                 split_ratio_evaluate, split_ratio_test,\n",
    "                                                 train_dir, evaluate_dir, test_dir, bucket_name,\n",
    "                                                 \"dataset/extract/\", s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87945e0d-82f7-4a41-aaec-a19a23b4a822",
   "metadata": {},
   "source": [
    "# Train & evaluate ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf618a-2ef4-4788-b3d7-4408facc2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080d297-07d6-4739-a3d1-5cf960f3dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"final_model.keras\"\n",
    "model_plot_filename = \"model_plot.png\"\n",
    "batch_size = 64 \n",
    "epochs = 4\n",
    "\n",
    "# train & evaluate\n",
    "model_dir = working_dir + \"/model\"\n",
    "model_path = model_dir + \"/\" + model_filename\n",
    "plot_filepath = model_dir + \"/\" + model_plot_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5039e4e-433a-4856-bf53-2745120bdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from keras import Model\n",
    "from keras.src.applications.vgg16 import VGG16\n",
    "from keras.src.callbacks import History\n",
    "from keras.src.layers import Dropout, Flatten, Dense\n",
    "from keras.src.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def train_and_evaluate_model(train_dir: str,\n",
    "                             evaluate_dir: str,\n",
    "                             test_dir: str,\n",
    "                             model_dir: str,\n",
    "                             model_path: str,\n",
    "                             plot_filepath: str,\n",
    "                             batch_size: int,\n",
    "                             epochs: int):\n",
    "    model = define_model()\n",
    "\n",
    "    # create data generator\n",
    "    datagen = ImageDataGenerator(featurewise_center=True)\n",
    "    # specify imagenet mean values for centering\n",
    "    datagen.mean = [123.68, 116.779, 103.939]\n",
    "    # prepare iterator\n",
    "    train_it = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        class_mode=\"binary\",\n",
    "        batch_size=batch_size,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    validation_it = datagen.flow_from_directory(\n",
    "        evaluate_dir,\n",
    "        class_mode=\"binary\",\n",
    "        batch_size=batch_size,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    # fit model\n",
    "    history = model.fit(\n",
    "        train_it,\n",
    "        steps_per_epoch=len(train_it),\n",
    "        validation_data=validation_it,\n",
    "        validation_steps=len(validation_it),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    # test model\n",
    "    evaluate_it = datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        class_mode=\"binary\",\n",
    "        batch_size=batch_size,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    _, acc = model.evaluate(evaluate_it, steps=len(evaluate_it), verbose=1)\n",
    "    evaluate_accuracy_percentage = acc * 100.0\n",
    "    print(\"> %.3f\" % evaluate_accuracy_percentage)\n",
    "\n",
    "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    create_history_plots(history, plot_filepath)\n",
    "\n",
    "    model.save(model_path)\n",
    "\n",
    "def define_model() -> Model:\n",
    "    model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    "    # mark loaded layers as not trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add new classifier layers\n",
    "    output = model.layers[-1].output\n",
    "    drop1 = Dropout(0.2)(output)\n",
    "    flat1 = Flatten()(drop1)\n",
    "    class1 = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")(flat1)\n",
    "    output = Dense(3, activation=\"sigmoid\")(class1)\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_history_plots(history: History, plot_filepath: str):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title(\"Cross Entropy Loss\")\n",
    "    pyplot.plot(history.history[\"loss\"], color=\"blue\", label=\"train\")\n",
    "    pyplot.plot(history.history[\"val_loss\"], color=\"orange\", label=\"test\")\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title(\"Classification Accuracy\")\n",
    "    pyplot.plot(history.history[\"accuracy\"], color=\"blue\", label=\"train\")\n",
    "    pyplot.plot(history.history[\"val_accuracy\"], color=\"orange\", label=\"test\")\n",
    "    # save plot to file\n",
    "    pyplot.savefig(plot_filepath)\n",
    "    pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03bb50c-6e97-472f-9ee1-1c71942f00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model(train_dir, evaluate_dir, test_dir, model_dir, model_path,\n",
    "                         plot_filepath, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26df797-6d10-4c42-bda6-16207e9bce83",
   "metadata": {},
   "source": [
    "# Test the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fc058-9765-4b65-98f5-04513f8c674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename: str|BytesIO):\n",
    "    # load the image\n",
    "    img = load_img(filename, target_size=(224, 224))\n",
    "    # convert to array\n",
    "    img = img_to_array(img)\n",
    "    # reshape into a single sample with 3 channels\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    # center pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img - [123.68, 116.779, 103.939]\n",
    "    return img\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model = load_model(model_path)\n",
    "\n",
    "    def execute(self, filepath:str|BytesIO):\n",
    "        img = load_image(filepath)\n",
    "        result = self.model.predict(img)\n",
    "        values = [float(result[0][0]), float(result[0][1]), float(result[0][2])]\n",
    "        switcher = ['Cat', 'Dog', 'Other']\n",
    "        prediction = np.argmax(result[0])\n",
    "        return {\"prediction\": switcher[prediction], \"values\": values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd021f4-e992-4c07-88aa-287cb34db848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def test_model(model_inference: Inference, model_dir: str, test_dir: str):\n",
    "    statistics = {\"ok\": 0, \"ko\": 0, \"total\": 0}\n",
    "    results = []\n",
    "    path_test_dir = Path(test_dir)\n",
    "    for path in path_test_dir.glob(\"**/*\"):\n",
    "        if path.is_dir():\n",
    "            continue\n",
    "        model_result = model_inference.execute(str(path))\n",
    "\n",
    "        prediction = model_result[\"prediction\"]\n",
    "        prediction_truth = path.parent.name.lower().replace(\"s\", \"\")\n",
    "        status = prediction_truth == prediction.lower()\n",
    "        statistics[\"ok\" if status else \"ko\"] += 1\n",
    "        result = {\n",
    "            \"filename\": path.name,\n",
    "            \"ok\": status,\n",
    "            \"prediction\": prediction,\n",
    "            \"prediction_truth\": prediction_truth,\n",
    "            \"values\": model_result[\"values\"],\n",
    "        }\n",
    "        results.append(result)\n",
    "    statistics[\"total\"] = statistics[\"ok\"] + statistics[\"ko\"]\n",
    "\n",
    "    with open(model_dir + \"/statistics.json\", \"w\") as file_stream:\n",
    "        json.dump(statistics, file_stream, indent=4)\n",
    "\n",
    "    with open(model_dir + \"/predictions.json\", \"w\") as file_stream:\n",
    "        json.dump(results, file_stream, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375371a-5543-4951-807e-ee3453361019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "model_inference = Inference(model_path)\n",
    "\n",
    "test_model(model_inference, model_dir, test_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
